# 1 可编程离线批同步

将json数据通过python进行复杂etl处理

> 使用前，请先在环境中安装pyjava. 尝试使用 pip install pyjava命令。
> pyjava会提供一个叫data_manager的变量，方便接受和返回数据给MLSQL主程序。
> 主要有两个方法：
>    获取数据， data_manager.fetch_once(), 返回一个迭代器，注意，该方法只能调用一次。
>    设置返回数据， data_manager.set_output(value) value格式必须是 [[pandas.serial,pandas.serial,...]]


# 2 离线etl处理数据

第一步：首先先定义一个jsonstr的reader，读取json字符串数据载入表中
```sql
set rawText='''
{"id":"1101","name":"小明1","age":20,"message":"testmsg1","date":"20210112","version":1}
{"id":"1102","name":"小明2","age":21,"message":"testmsg2","date":"20210112","version":1}
{"id":"1103","name":"小明3","age":22,"message":"testmsg3","date":"20210112","version":1}
{"id":"1104","name":"小明4","age":23,"message":"testmsg4","date":"20210112","version":1}
{"id":"1105","name":"小明5","age":24,"message":"testmsg5","date":"20210112","version":1}
{"id":"1106","name":"小明6","age":25,"message":"testmsg6","date":"20210112","version":1}
{"id":"1107","name":"小明7","age":26,"message":"testmsg7","date":"20210112","version":1}
{"id":"1108","name":"小明8","age":27,"message":"testmsg8","date":"20210112","version":1}
{"id":"1109","name":"小明9","age":28,"message":"testmsg9","date":"20210112","version":1}
{"id":"1110","name":"小明10","age":29,"message":"testmsg10","date":"20210112","version":2}
''';

load jsonStr.`rawText` as orginal_text_corpus;
```

第二步：读取上一步生成的表orginal_text_corpus的结构，使用python做etl操作的话，输出的StructType必须和源表一致。

```sql
!show schema orginal_text_corpus;
```

读取后可以得到每个列的type，可以得到如下的输出，可以看到age,date,id,message,name,version的类型分别是long,string,string,string,string和long

```json
[{"schema":"root\n |-- age: long (nullable = true)\n |-- date: string (nullable = true)\n |-- id: string (nullable = true)\n |-- message: string (nullable = true)\n |-- name: string (nullable = true)\n |-- version: long (nullable = true)\n"}]
```

第四步：定义python环境变量以及schema
1. 在本机安装了多版本的python，所以需要指定需要使用的python的执行路径，这里用python.bin.path来定义。
2. 然后定义schema参数，使用st(field(name,type)...)这种方式定义，使用上一步获得的schema类型填充，注意类型必须一致，不然会报错读不出数据。
```
!python conf "python.bin.path=/Library/Frameworks/Python.framework/Versions/3.6/bin/python3 ";
!python conf "schema=st(field(id,string),field(name,string),field(age,long),field(message,string),field(date,string),field(version,long))";
```

第五步：定义python处理逻辑，并将结果输出到表out_temp_table中。

这里看代码做了很简单的字符串拼接工作。除了这个简单用法外，还可以执行更多复杂的操作。比如满足某些条件实时告警，也可以执行机器学习的预测等高级用法。

```
!python on orginal_text_corpus '''

data = context.fetch_once_as_rows()
def process(data):
    for row in data:
        new_row = { }
        new_row["id"] = row["id"]
        new_row["name"] = "---" + row["name"]+"---"
        new_row["age"] = row["age"]
        new_row["message"] = "---" + row["message"]+"---"
        new_row["date"] = row["date"]
        new_row["version"] = row["version"]
        print("test:"+str(new_row))
        yield new_row

context.build_result(process(data))

''' named out_temp_table;
```

第六步：查询结果

```
select * from out_temp_table as output;
```

```json
[
    {
        "id":"1101",
        "name":"---小明1---",
        "age":20,
        "message":"---testmsg1---",
        "date":"20210112",
        "version":1
    },
    {
        "id":"1102",
        "name":"---小明2---",
        "age":21,
        "message":"---testmsg2---",
        "date":"20210112",
        "version":1
    },
    {
        "id":"1103",
        "name":"---小明3---",
        "age":22,
        "message":"---testmsg3---",
        "date":"20210112",
        "version":1
    },
    {
        "id":"1104",
        "name":"---小明4---",
        "age":23,
        "message":"---testmsg4---",
        "date":"20210112",
        "version":1
    },
    {
        "id":"1105",
        "name":"---小明5---",
        "age":24,
        "message":"---testmsg5---",
        "date":"20210112",
        "version":1
    },
    {
        "id":"1106",
        "name":"---小明6---",
        "age":25,
        "message":"---testmsg6---",
        "date":"20210112",
        "version":1
    },
    {
        "id":"1107",
        "name":"---小明7---",
        "age":26,
        "message":"---testmsg7---",
        "date":"20210112",
        "version":1
    },
    {
        "id":"1108",
        "name":"---小明8---",
        "age":27,
        "message":"---testmsg8---",
        "date":"20210112",
        "version":1
    },
    {
        "id":"1109",
        "name":"---小明9---",
        "age":28,
        "message":"---testmsg9---",
        "date":"20210112",
        "version":1
    },
    {
        "id":"1110",
        "name":"---小明10---",
        "age":29,
        "message":"---testmsg10---",
        "date":"20210112",
        "version":2
    }
]
```

# 3 结合numpy+pandas创建数据

使用python嵌入语句的功能结合numpy+pandas，可以做很多有趣的事情，这里举一个例子是创造数据。

在没有python嵌入语句之前，创建数据一般是使用jsonstr，csvstr或者先把数据写入文件，上传到oss或hdfs中，再读取类似这种方式。

前者需要写很多冗长的json，后者又比较麻烦。

有了numpy+pandas可以通过写几行代码模拟生成数据。

参考下面的例子：

第一步：首先建一个临时表，这一步没有什么实际的意义，目前python批的语法需要基于一个临时表。

```sql
select 1 as orginal_corpus;
```

第二步：定义python环境变量以及输出schema。

1. 本人是多python环境，所以需要指定python环境，以免使用错python环境。
2. 定义输出schema，这里输出的列为c1,c2,c3，类型分别是double,double和string。
这里有个tip，输出的列可以不用和后序pandas的dataframe中的serise列名一致，只要列对齐即可，当然类型需要一致。

```sql
!python conf "python.bin.path=/Library/Frameworks/Python.framework/Versions/3.6/bin/python3 ";
!python conf "schema=st(field(c1,double),field(c2,double),field(c3,string))";
```

第三步：书写生成数据的python代码

这一步是核心逻辑，使用np.random.randn(4)创建4个随机数，np.linspace 创建浮点数等差数列，第三个列定义4个字符串。
最后使用data_manager.set_output([[df['one'],df['two'],df['three']]])将结果回传。


```sql
!python on orginal_corpus '''
import pandas as pd
import numpy as np
data={"one":np.random.randn(4),"two":np.linspace(1,4,4),"three":['zhangsan','李四','999','0.1']}
df=pd.DataFrame(data)
print([[df['one'],df['two'],df['three']]])

data_manager.set_output([[df['one'],df['two'],df['three']]])

''' named out_temp_table;
```

第四步：查询返回结果

```sql
select * from out_temp_table;
```

```
[
    {
        "c1":-1.0251249561401554,
        "c2":1,
        "c3":"zhangsan"
    },
    {
        "c1":-0.3086510830547062,
        "c2":2,
        "c3":"李四"
    },
    {
        "c1":-0.8966636035221165,
        "c2":3,
        "c3":"999"
    },
    {
        "c1":-0.7216453697254857,
        "c2":4,
        "c3":"0.1"
    }
]
```

# 4 其他注意点

1. 若在python中使用dict，需要{ } 这样指定，中间空一个空格，不然会识别到字段参数替换，出问题。
2. dict字段顺序，pandas的dataframe字段顺序与schema中一致。
3. python中定义的列的类型必须是同一类型，即不能有这样的类型，比如`"three":['zhangsan','李四',999,0.1]`，可以看到这里既有string，
又有double和long，这样是不行的。arrow必须要有确定的类型，不能是object类型。
4. 三种类型：目前输出只有三种类型，分别是string,double和long类型。
5. 如果在使用过程中出现无返回，多半是类型映射错误，这种时候有两种检测手段。
如果是从spark的表中读取，可以调用`!show schema orginal_text_corpus;` 语法获得每个列的类型。
如果是从python中读取，需要在本地python环境打印一下，这个后序版本会优化。
6. 如果需要对python中的列进行类型转换，可以参考下面这种用法。加上int() 强转即可，类似也可以用float(),str() 
注意，不管怎么强转，最终类型就是string,double和long这三种类型。
```sql
set rawText='''
{"id":"1101"}
{"id":"1102"}
{"id":"1103"}
''';

!python conf "schema=st(field(id,long))"

!python on orginal_text_corpus '''

data = context.fetch_once_as_rows()
def process(data):
    for row in data:
        new_row = { }
        new_row["id"] = int(row["id"])
        yield new_row

context.build_result(process(data))

''' named out_temp_table;
```

